import pandas as pd
import statsmodels.api as sm
from patsy import dmatrices
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_curve, auc
from statsmodels.nonparametric.kde import KDEUnivariate

# Step 1: Load the dataset
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
df = pd.read_csv(url)
pd.set_option('display.max_columns', None)  # None means no limit
print(df.head())

# Step 2: Clean the data
# Drop columns that won't add value to the model
df = df.drop(['Ticket', 'Cabin', 'Name'], axis=1)
# Remove rows with missing values
df = df.dropna()

# Step 3: Create training and test sets
# Create the formula for dependent and independent variables
formula = 'Survived ~ C(Pclass) + C(Sex) + Age + SibSp + C(Embarked) + Parch'
"""
    it is a patsy formula notation commonly used for specifying linear models where it declares the DV and IVs 
    to be used in the linear model.
"""
# Split the data into training and test sets
df_train = df.iloc[0:600, :]  # For training the model
df_test = df.iloc[600:, :]  # For testing the model

# Feed y_train, x_train, y_test, and x_test using dmatrices
y_train, x_train = dmatrices(formula, data=df_train, return_type='dataframe') 
y_test, x_test = dmatrices(formula, data=df_test, return_type='dataframe')

# Step 4: Build the logistic regression model
model = sm.Logit(y_train, x_train)  # sm = statsmodels, logit = logistic regression
res = model.fit()

# Display the summary of the model
print(res.summary())

# Step 5: Evaluate the model 
# Predictions on training data (use KDEUnivariate to plot the probabilities of passengers' survival in the Titanic dataset)
kde_res = KDEUnivariate(res.predict()) 
"""
    res.predict(): predicted probabilities generated by the logistic regression model
    KDEUnivariate: Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a continuous random variable.
"""
kde_res.fit()  # The fit() method estimates the density function based on the data provided.
plt.plot(kde_res.support, kde_res.density)
"""
    support indicates the specific values at which the density is calculated.
    for example, you have a dataset of ages ranging from 10 to 80 years.
    The density values correspond to the estimated probability density function at each point in the support.
"""
x_lines = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

# Add vertical lines at specified x-coordinates
for x in x_lines:
    plt.axvline(x=x, color='red', linestyle='--', label=f'Predicted Probability = {x}')
# Fill the area under the curve
plt.fill_between(kde_res.support, kde_res.density, alpha=0.2)
"""
    This function fills the area between the support and the density curve with a specified transparency
    (controlled by alpha). This makes the plot visually clearer by highlighting the area under the curve.
"""
plt.title("Distribution of Predictions")
plt.show()

# Scatter plot for Male Gender
plt.scatter(res.predict(), x_train['C(Sex)[T.male]'], alpha=0.2) 
"""
    res.predict() = to get the predicted probabilities of survival for each passenger in the training dataset.
    C(Sex)[T.male] is a dummy variable created from the categorical Sex feature, where:
    If a passenger is male, the value is 1.
    If a passenger is female, the value is 0.
"""
plt.grid(visible=True, which='major', axis='x')
"""
    b=True: Enables the grid.
    which='major': Specifies that major grid lines should be drawn.
    axis='x': Indicates that the grid should be drawn along the x-axis (the predicted survival probabilities).
"""
plt.xlabel("Predicted chance of survival")
plt.ylabel("Male Gender")
plt.title("Survival Probability by Gender")
plt.show()

# Scatter plot for Passenger Class
plt.scatter(res.predict(), x_train['C(Pclass)[T.3]'], alpha=0.2)
# T.3 means third passenger class in Titanic dataset
plt.xlabel("Predicted chance of survival")
plt.ylabel("3rd Class Bool")
plt.grid(visible=True, which='major', axis='x')
plt.title("Survival Probability by 3rd Class")
plt.show()

# Scatter plot for Age
plt.scatter(res.predict(), x_train.Age, alpha=0.2)
plt.grid(visible=True, linewidth=0.15)
plt.title("Survival Probability by Age")
plt.xlabel("Predicted chance of survival")
plt.ylabel("Age")
plt.show()

# Scatter plot for Siblings/Spouses
plt.scatter(res.predict(), x_train.SibSp, alpha=0.2)
plt.grid(visible=True, linewidth=0.15)
plt.title("Survival Probability by Number of Siblings/Spouses")
plt.xlabel("Predicted chance of survival")
plt.ylabel("No. of Siblings/Spouses")
plt.show()

# Evaluating model on test data
y_pred = res.predict(x_test)
print(y_pred)
y_pred_flag = y_pred > 0.7
print(y_pred_flag)

# Confusion matrix (shown through the contingency table)
print(pd.crosstab(y_test.Survived, y_pred_flag, rownames=['Actual'], colnames=['Predicted']))
"""
    y_test.Survived will form the rows of the crosstab (1 for survived and 0 for not survived)
"""

# Classification report
print('\nClassification Report:\n', classification_report(y_test, y_pred_flag))

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)  # fpr = false positive rate, tpr = true positive rate
print(fpr)
print(tpr)  
roc_auc = auc(fpr, tpr)  # roc = Receiver Operating Characteristic, auc = area under curve
print(roc_auc)
print("Area under the ROC curve: %f" % roc_auc)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
"""
    This line adds a dashed diagonal line ('k--' specifies a black dashed line) from (0, 0) to (1, 1).
    This diagonal represents a model that performs no better than random guessing (i.e., AUC = 0.5).
    If the ROC curve lies above this line, the model is better than random.
"""
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
"""
    These lines set the x-axis and y-axis limits from 0 to 1, ensuring that the plot scales correctly
    for ROC data.
"""
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
